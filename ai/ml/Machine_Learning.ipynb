{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAd3NLB_Dfmx"
      },
      "source": [
        "# Machine Learning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TwWj-1U8BrAv"
      },
      "source": [
        "# Introduction to ML\n",
        "\n",
        "Machine learning is a field of inquiry devoted to understanding and building methods that __'learn'__, that is, methods that leverage data to improve performance on some set of tasks. It is seen as a part of artificial intelligence.\n",
        "\n",
        "## Additional information\n",
        "\n",
        "* _Rules of Machine Learning,_ [Rule #1: Don't be afraid to launch a product without machine learning](https://developers.google.com/machine-learning/rules-of-ml/#rule_1_dont_be_afraid_to_launch_a_product_without_machine_learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Framing: Key ML Terminology\n",
        "\n",
        "What is (supervised) machine learning? Concisely put, it is the following:\n",
        "\n",
        "* ML systems learn how to combine input to produce useful predictions on never-before-seen data.\n",
        "\n",
        "Let's explore fundamental machine learning terminology.\n",
        "\n",
        "## Labels\n",
        "\n",
        "A __label__ is the thing we are predicting - the `y` variable in simple linear regression. The label could be the future price of wheat, the kind of animal shown in the picture, the meaning of an audio clip, or just about anything.\n",
        "\n",
        "## Features\n",
        "\n",
        "A __feature__ is an input variable - the `x` variable in simple linera regression. A simple machine learning project might use a single feature, while a more sophisticated machine learning project could use millions of features, specified as:\n",
        "\n",
        "$$x1,x2,...,xn$$\n",
        "\n",
        "In the \"spam detector\" example, the features could include the following:\n",
        "\n",
        "* words in the email text\n",
        "* sender's address\n",
        "* time of day the email was sent\n",
        "* email contains the phrase \"one weird trick.\"\n",
        "\n",
        "## Examples\n",
        "\n",
        "An __example__ is a particular instance of data, __x__. (We put __x__ in boldface to indicate that it is a vector.) We break __examples__ into two categories:\n",
        "\n",
        "* labeled examples\n",
        "* unlabeled examples\n",
        "\n",
        "A __labeled example__ includes both feature(s) and the label. That is:\n",
        "\n",
        "```\n",
        "labeled examples: {features, label}: (x, y)\n",
        "```\n",
        "\n",
        "Use __labeled examples__ to train the model. In our \"span detector\" example, the labeled examples would be individual emails that users have explicitly marked as \"spam\" or \"not spam.\"\n",
        "\n",
        "For example, the following table shows 5 labeled examples from a data set containing information about housing prices in California:\n",
        "\n",
        "__HousingMedianAge (feature)__ | __TotalRooms (feature)__ | __TotalBedrooms (feature)__ | __MedianHouseValue (feature)__\n",
        ":--|:--:|:--:|--:\n",
        "15 | 5612 | 1283 | 66900\n",
        "19 | 7650 | 1901 | 80100\n",
        "17 | 720 | 174 | 85700\n",
        "14 |\t1501 |\t337\t| 73400\n",
        "20\t| 1454 |\t326 |\t65500\n",
        "\n",
        "An __unlabeled example__ contains feature(s) but not the label. That is:\n",
        "\n",
        "```\n",
        "unlabeled examples: {features, ?}: (x, ?)\n",
        "```\n",
        "\n",
        "Here are 3 unlabeled examples from the same housing dataset, which exclude __`MedianHouseValue`__:\n",
        "\n",
        "__HousingMedianAge (feature)__ |\t__TotalRooms (feature)__ |\t__TotalBedrooms (feature)__\n",
        ":--|:--:|--:\n",
        "42 |\t1686 |\t361\n",
        "34 |\t1226 |\t180\n",
        "33 |\t1077 |\t271\n",
        "\n",
        "Once we've trained our model with labeled examples, we use that model to predict the label on unlabeled examples. In the spam detector, unlabeled examples are new emails that humans haven't yet labeled.\n",
        "\n",
        "## Models\n",
        "\n",
        "A model defines the relationship between feature(s) and label. For example, a span detector might associate certain features strongly with \"spam\". Let's highlight two phases of a model's life:\n",
        "\n",
        "* __Training__ means __creating__ or __learning__ the model. That is, you show the model labeled examples and enable the model to gradually learn the relationships between features and label.\n",
        "\n",
        "* __Inference__ means applying the trained model to unlabeled examples. That is, you use the trained model to make useful predictions (`y'`). For example, during __inference__, you can predict __MedianHouseValue__ for new unlabeled examples.\n",
        "\n",
        "## Regression vs. classification\n",
        "\n",
        "A __regression__ model predicts continuous values. For example, regression models make predictions that answer questions like the following:\n",
        "\n",
        "* What is the value of a house in California?\n",
        "* What is the probability that a user will click on this ad?\n",
        "\n",
        "A __classification__ model predicts discrete values. For example, classification models make predictions that answer questions like the following:\n",
        "\n",
        "* Is a given email message spam or not spam?\n",
        "* Is this an image of a dog, a cat, or a hamster?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check your understanding\n",
        "\n",
        "__Supervised Learning__\n",
        "\n",
        "__Q1. Suppose you want to develop a supervised machine learning model to predict whether a given email is \"spam\" or \"not spam.\" What are the true statements that you can think about for being a useful label?__\n",
        "\n",
        "> * __Emails not marked as \"spam\" or \"not spam\" are unlabeled examples.__ <br>\n",
        ">    Because our label consists of the values \"spam\" and \"not spam\", any email not yet marked as spam or not spam is an unlabeled example.\n",
        ">\n",
        "> * __The labels applied to some examples might be unreliable.__ <br>\n",
        ">    Definitely. It's important to check how reliable your data is. The labels for this dataset probably come from email users who mark particular email messages as spam. Since most users do not mark every suspicious email message as spam, we may have trouble knowing whether an email is spam. Furthermore, spammers could intentionally poison our model by providing faulty labels.\n",
        "\n",
        "__Features and Labels__\n",
        "\n",
        "__Q1. Suppose an online shoe store wants to create a supervised ML model that will provide personalized shoe recommendations to users. That is, the model will recommend certain pairs of shoes to Marty and different pairs of shoes to Janet. The system will use past user behavior data to generate training data. What are the true statements that you can think of for being a useful label?__\n",
        "\n",
        "> * __\"Shoe size\" is a useful feature.__ <br>\n",
        "> \"Shoe size\" is a quantifiable signal that likely has a strong impact on whether the user will like the recommended shoes. For example, if Marty wears size 9, the model shouldn't recommend size 7 shoes.\n",
        ">\n",
        "> * __\"The user clicked on the shoe's description\" is a useful label.__ <br>\n",
        "> Users probably only want to read more about those shoes that they like. Clicks by users is, therefore, an observable, quantifiable metric that could serve as a good training label. Since our training data derives from past user behavior, our labels need to derive from objective behaviors like clicks that strongly correlate with user preferences.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Descending into ML\n",
        "\n",
        "Linear regression is a method for finding the straight line or hyperplane that best fits a set of points."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Regression\n",
        "\n",
        "It has long been known that crickets (an insect species) chirp more frequently on hotter days than on cooler days. For decades, professional and amateur scientists have cataloged data on chirps-per-minute and temperature. As a birthday gift, your Aunt Ruth gives you her cricket database and asks you to learn a model to predict this relationship. Using this data, you want to explore this relationship.\n",
        "\n",
        "First, examine your data by plotting it:\n",
        "\n",
        "<div align='center'>\n",
        "  <img src='https://developers.google.com/static/machine-learning/crash-course/images/CricketPoints.svg' />\n",
        "\n",
        "  <strong>Figure 1. Chirps per Minute vs. Temperature in Celsius.</strong>\n",
        "</div>\n",
        "\n",
        "As expected, the plot shows the temperature rising with the number of chirps. Is this relationship between chirps and temperature linear? Yes, you could draw a single straight line like the following to approximate this relationship:\n",
        "\n",
        "<div align='center'>\n",
        "  <img src='https://developers.google.com/static/machine-learning/crash-course/images/CricketLine.svg' />\n",
        "\n",
        "  <strong>Figure 2. A linear relationship.</strong>\n",
        "</div>\n",
        "\n",
        "True, the line doesn't pass through every dot, but the line does clearly show the relationship between chirps and temperature. Using the equation for a line, you could write down this relationship as follows:\n",
        "\n",
        "$$y=mx+b$$\n",
        "\n",
        "where:\n",
        "\n",
        "*  $y$ is the temperature in Celsius — the value we're trying to predict.\n",
        "* $m$ is the slope of the line.\n",
        "* $x$ is the number of chirps per minute — the value of our input feature.\n",
        "* $b$ is the y-intercept.\n",
        "\n",
        "By convention in machine learning, you'll write the equation for a model slightly differently:\n",
        "\n",
        "$$y^\\prime=b+w_{1}x_{1}$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $y^\\prime$ is a predicted [label](https://developers.google.com/machine-learning/crash-course/framing/ml-terminology#labels) (a desired output).\n",
        "* $b$ is the bias (the y-intercept), sometimes referred to as $w_{0}$.\n",
        "* $w_{1}$ is the [weight](https://developers.google.com/machine-learning/glossary#weight) of feature 1. Weight is the same concept as the \"slope\" _$m$_ in the traditional equation of a line.\n",
        "* $x_{1}$ is a [feature](https://developers.google.com/machine-learning/crash-course/framing/ml-terminology#features) (a known input).\n",
        "\n",
        "To __infer__ (predict) the temprature $y^\\prime$ for new chirps-per-minute value $x_{1}$, just substitute the $x_{1}$ value into this model.\n",
        "\n",
        "Although this model uses only one feature, a more sophisticated model might rely on multiple features, each having a separate weight ($w_{1}$, $w_{2}$, etc.). For example, a model that relies on three features might look as follows:\n",
        "\n",
        "$$y^\\prime=b+w_{1}x_{1}+w_{2}x_{2}+w_{3}x_{3}$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and Loss\n",
        "\n",
        "__Training__ a model simply means learning (determining) good values for all the weights and the bias from labeled examples. In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss; this process is called __empirical risk minimization__.\n",
        "\n",
        "Loss is the penalty for a bad prediction. That is, __loss__ is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater. The goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples. For example, Figure 3 shows a high loss model on the left and a low loss model on the right. Note the following about the figure:\n",
        "\n",
        "* The arrows represent loss.\n",
        "* The blue lines represent predictions.\n",
        "\n",
        "<div algin='center'>\n",
        "  <img src='https://developers.google.com/machine-learning/crash-course/images/LossSideBySide.png' />\n",
        "\n",
        "  <strong>Figure 3. High loss in the left model; low loss in the right model.</strong>\n",
        "</div>\n",
        "\n",
        "Notice that the arrows in the left plot are much longer than their counterparts in the right plot. Clearly, the line in the right plot is a much better predictive model than the line in the left plot.\n",
        "\n",
        "You might be wondering whether you could create a mathematical function — a loss function — that would aggregate the individual losses in a meaningful fashion."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Squared loss: a popular loss function\n",
        "\n",
        "The linear regression models we'll examine here use a loss function called __squared loss__ (also known as __L2 loss__). The __squared loss__ for a single example is as follows:\n",
        "\n",
        "```\n",
        "  = the square of the difference between the label and the prediction\n",
        "  = (observation - prediction(x))2\n",
        "  = (y - y')2\n",
        "```\n",
        "\n",
        "__Mean square error (MSE)__ is the average squared loss per example over the whole dataset. To calculate __MSE__, sum up all the squared losses for individual examples and then divide by the number of examples:\n",
        "\n",
        "$$\\mathrm{MSE}=\\dfrac{1}{N}\\sum_{(x,y) \\in D} (y - \\mathrm{prediction}(x))^2$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $(x,y)$ is an example in which\n",
        "  * $x$ is a set of features (for example chirps/minute, age, gender) that the model uses to make predictions.\n",
        "  * $y$ is the example's label (for example, temprature).\n",
        "* $prediction(x)$ is a function of the weights and bias in combination with the sets of features $x$.\n",
        "* $D$ is a dataset containing many labeled examples, which are $(x,y)$ pairs.\n",
        "* $N$ is the number of examples in $D$.\n",
        "\n",
        "Although MSE is commonly-used in machine learning, it is neither the only practical loss function nor the best loss function for all circumstances."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check your understanding\n",
        "\n",
        "__Mean Squared Error__\n",
        "\n",
        "Consider the following two plots:\n",
        "\n",
        "<div align='center'>\n",
        "  <img src='https://developers.google.com/static/machine-learning/crash-course/images/MCEDescendingIntoMLLeft.png' />\n",
        "\n",
        "  <img src='https://developers.google.com/static/machine-learning/crash-course/images/MCEDescendingIntoMLRight.png' />\n",
        "</div>\n",
        "\n",
        "__Q1. Which of the two data sets shown in the preceding plots has the higher Mean Squared Error (MSE)?__\n",
        "\n",
        "> __The dataset on the right.__ <br>\n",
        "> The eight examples on the line incur a total loss of 0. However, although only two points lay off the line, both of those points are twice as far off the line as the outlier points in the left figure. Squared loss amplifies those differences, so an offset of two incurs a loss four times as great as an offset of one.\n",
        "> $$\\mathrm{MSE}=\\dfrac{0^2+0^2+0^2+2^2+0^2+0^2+0^2+2^2+0^2+0^2}{10}=0.8$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reducing Loss\n",
        "\n",
        "To train a model, we need a good way to reduce the model’s loss. An iterative approach is one widely used method for reducing loss, and is as easy and efficient as walking down a hill."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## An iterative approach\n",
        "\n",
        "Iterative learning might remind you of the \"[Hot and Cold](http://www.howcast.com/videos/258352-how-to-play-hot-and-cold/)\" kid's game for finding a hidden object like a thimble. In this game, the \"hidden object\" is the best possible model. You'll start with a wild guess (\"The value of $w_{1}$ is $0$.\") and wait for the system to tell you what the loss is. Then, you'll try another guess (\"The value of $w_{1}$ is $0.5$.\") and see what the loss is. Aah, you're getting warmer. Actually, if you play this game right, you'll usually be getting warmer. The real trick to the game is trying to find the best possible model as efficiently as possible.\n",
        "\n",
        "The following figure suggests the iterative trial-and-error process that machine learning algorithms use to train a model:\n",
        "\n",
        "<div align='center'>\n",
        "  <img src=\"https://developers.google.com/static/machine-learning/crash-course/images/GradientDescentDiagram.svg\" />\n",
        "\n",
        "  <strong>Figure 1. An iterative approach to training a model.</strong>\n",
        "</div>\n",
        "\n",
        "Iterative strategies are prevalent in machine learning, primarily because they scale so well to large data sets.\n",
        "\n",
        "The \"model\" takes one or more features as input and returns one prediction ($y′$) as output. To simplify, consider a model that takes one feature and returns one prediction:\n",
        "\n",
        "$$y′=b+w_{1}x_{1}$$\n",
        "\n",
        "What initial values should we set for $b$ and $w_{1}$? For linear regression problems, it turns out that the starting values aren't important. We could pick random values, but we'll just take the following trivial values instead:\n",
        "\n",
        "* $b=0$\n",
        "* $w_{1}=0$\n",
        "\n",
        "Suppose that the first feature value is 10. Plugging that feature value into the prediction function yields:\n",
        "\n",
        "$$y′=0+0⋅10=0$$\n",
        "\n",
        "The \"Compute Loss\" part of the diagram is the [loss function](https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss) that the model will use. Suppose we use the squared loss function. The loss function takes in two input values:\n",
        "\n",
        "* $y′$: The model's prediction for features $x$\n",
        "* $y$: The correct label corresponding to features $x$\n",
        "\n",
        "At last, we've reached the \"Compute parameter updates\" part of the diagram. It is here that the machine learning system examines the value of the loss function and generates new values for $b$ and $w_{1}$. For now, just assume that this mysterious box devises new values and then the machine learning system re-evaluates all those features against all those labels, yielding a new value for the loss function, which yields new parameter values. And the learning continues iterating until the algorithm discovers the model parameters with the lowest possible loss. Usually, you iterate until overall loss stops changing or at least changes extremely slowly. When that happens, we say that the model has [__converged__](https://developers.google.com/machine-learning/glossary#convergence)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradient Descent\n",
        "\n",
        "The iterative approach diagram contained a green hand-wavy box entitled \"Compute parameter updates.\" We'll now replace that algorithmic fairy dust with something more substantial."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Suppose we had the time and the computing resources to calculate the loss for all possible values of $w_{1}$. For the kind of regression problems we've been examining, the resulting plot of loss vs. $w_{1}$ will always be convex. In other words, the plot will always be bowl-shaped, kind of like this:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align='center'>\n",
        "  <img src=\"https://developers.google.com/static/machine-learning/crash-course/images/convex.svg\" />\n",
        "\n",
        "  <strong>Figure 2. Regression problems yield convex loss vs. weight plots.</strong>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convex problems have only one minimum; that is, only one place where the slope is exactly 0. That minimum is where the loss function converges.\n",
        "\n",
        "Calculating the loss function for every conceivable value of $w_{1}$ over the entire data set would be an inefficient way of finding the convergence point. Let's examine a better mechanism—very popular in machine learning—called __gradient descent__.\n",
        "\n",
        "The first stage in gradient descent is to pick a starting value (a starting point) for $w_{1}$. The starting point doesn't matter much; therefore, many algorithms simply set $w_{1}$ to $0$ or pick a random value. The following figure shows that we've picked a starting point slightly greater than $0$:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align='center'>\n",
        "  <img src='https://developers.google.com/static/machine-learning/crash-course/images/GradientDescentStartingPoint.svg' />\n",
        "  \n",
        "  <strong>Figure 3. A starting point for gradient descent.</strong>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The gradient descent algorithm then calculates the gradient of the loss curve at the starting point. Here in this Figure, the gradient of the loss is equal to the [derivative](https://wikipedia.org/wiki/Differential_calculus#The_derivative) (slope) of the curve, and tells you which way is \"warmer\" or \"colder.\" When there are multiple weights, the gradient is a vector of partial derivatives with respect to the weights."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Partial derivatives\n",
        "\n",
        "A __multivariable function__ is a function with more than one argument, such as:\n",
        "\n",
        "$$f(x,y)=e^{2y}\\mathrm{sin}(x)$$\n",
        "\n",
        "The __partial derivate__ $f$ __with respect to__ $x$, denoted as follows:\n",
        "\n",
        "$$\\dfrac{∂f}{∂x}$$\n",
        "\n",
        "is a derivative of $f$ considered as a function of $x$ alone. To find the following:\n",
        "\n",
        "$$\\dfrac{∂f}{∂x}$$\n",
        "\n",
        "so must hold $y$ constant (so $f$ is now a function of one variable $x$), and take the regular derivative of $f$ with respect to $x$. For example, when $y$ is fixed at $1$, the preceding function becomes:\n",
        "\n",
        "$$f(x)=e^{2}\\mathrm{sin}(x)$$\n",
        "\n",
        "This is just a function of one variable $x$, whose derivative is:\n",
        "\n",
        "$$e^{2}\\mathrm{cos}(x)$$\n",
        "\n",
        "In general, thinking of $y$ as fixed, the partial derivative of $f$ with respect to $x$ is calculated as follows:\n",
        "\n",
        "$$\\dfrac{∂f}{∂x}(x,y)=e^{2y}\\mathrm{sin}(x)$$\n",
        "\n",
        "Similarly, if we hold $x$ fixed instead, the partial derivative of $f$ with respect to $y$ is:\n",
        "\n",
        "$$\\dfrac{∂f}{∂x}(x,y)=2e^{2y}\\mathrm{sin}(x)$$\n",
        "\n",
        "Intuitively, a partial derivative tells how much the function changes when you perturb one variable a bit. In the preceding example:\n",
        "\n",
        "$$\\dfrac{\\partial f}{\\partial x}(0,1)=e^2 \\approx 7.4$$\n",
        "\n",
        "So when you start at $(0,1)$, hold $y$ constant, and move $x$ a little, $f$ changes by about $7.4$ times the amount that you changed $x$.\n",
        "\n",
        "In machine learning, partial derivatives are mostly used in conjunction with the gradient of a function."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gradients\n",
        "\n",
        "The __gradient__ of a function, denoted as follows, is the vector of partial derivatives with respect to all of the independent variables:\n",
        "\n",
        "$$∇f$$\n",
        "\n",
        "For instance, if:\n",
        "\n",
        "$$f(x,y)=e^{2y}\\mathrm{sin}(x)$$\n",
        "\n",
        "then:\n",
        "\n",
        "$$∇f(x,y)= \\left( \\dfrac{\\partial f}{\\partial x}(x,y), \\dfrac{\\partial f}{\\partial y}(x,y) \\right)=(e^{2y}\\mathrm{cos}(x), 2e^{2y}\\mathrm{sin}(x))$$\n",
        "\n",
        "Note the following:\n",
        "\n",
        "* $∇f$ - Points in the direction of greatest increase of the function.\n",
        "* $−∇f$ - Points in the direction of greatest decrease of the function.\n",
        "\n",
        "The number of dimensions in a vector is equal to the number of variables in the formula for $f$; in other words, the vector falls within the domain space of the function. For instance, the graph of the following function $f(x,y)$:\n",
        "\n",
        "$$f(x,y)=4+(x−2)^2+2y^2$$\n",
        "\n",
        "when viewed in three dimensions with $z=f(x,y)$ looks like a valley with a minimum at $(2,0,4)$:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align='center'>\n",
        "  <img src='https://developers.google.com/static/machine-learning/crash-course/images/ThreeDimensionalPlot.svg' />\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The gradient of $f(x,y)$ is a two-dimensional vector that tells you in which $(x,y)$ direction to move for the maximum increase in height. Thus, the negative of the gradient moves you in the direction of maximum decrease in height. In other words, the negative of the gradient vector points into the valley.\n",
        "\n",
        "In machine learning, gradients are used in gradient descent. We often have a loss function of many variables that we are trying to minimize, and we try to do this by following the negative of the gradient of the function."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Okay, so comming back to our gradient descent. We've found that a gradient is a vector that has both of the following characterstics:\n",
        "\n",
        "* A direction\n",
        "* A magnitude\n",
        "\n",
        "The gradient always points in the direction of steepest increase in the loss function. The gradient descent algorithm takes a step in the direction of the negative gradient in order to reduce loss as quickly as possible."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align='center'>\n",
        "  <img src='https://developers.google.com/static/machine-learning/crash-course/images/GradientDescentNegativeGradient.svg' />\n",
        "\n",
        "  <strong>Figure 4. Gradient descent relies on negative gradients.</strong>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To determine the next point along the loss function curve, the gradient descent algorithm adds some fraction of the gradient's magnitude to the starting point as shown in the following figure:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align='center'>\n",
        "  <img src='https://developers.google.com/static/machine-learning/crash-course/images/GradientDescentGradientStep.svg' />\n",
        "\n",
        "  <strong>Figure 5. A gradient step moves us to the next point on the loss curve.</strong>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The gradient descent then repeats this process, edging ever closer to the minimum."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Rate\n",
        "\n",
        "As noted, the gradient vector has both a direction and a magnitude. Gradient descent algorithms multiply the gradient by a scalar known as the __learning rate__ (also sometimes called __step size__) to determine the next point. For example, if the gradient magnitude is $2.5$ and the learning rate is $0.01$, then the gradient descent algorithm will pick the next point $0.025$ away from the previous point.\n",
        "\n",
        "__Hyperparameters__ are the knobs that programmers tweak in machine learning algorithms. Most machine learning programmers spend a fair amount of time tuning the learning rate. If you pick a learning rate that is too small, learning will take too long:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align='center'>\n",
        "  <img src='https://developers.google.com/static/machine-learning/crash-course/images/LearningRateTooSmall.svg' />\n",
        "\n",
        "  <strong>Figure 6. Learning rate is too small.</strong>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Conversely, if you specify a learning rate that is too large, the next point will perpetually bounce haphazardly across the bottom of the well like a quantum mechanics experiment gone horribly wrong:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align='center'>\n",
        "  <img src='https://developers.google.com/static/machine-learning/crash-course/images/LearningRateTooLarge.svg' />\n",
        "\n",
        "  <strong>Figure 7. Learning rate is too large.</strong>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There's a [Goldilocks](https://wikipedia.org/wiki/Goldilocks_principle) learning rate for every regression problem. The Goldilocks value is related to how flat the loss function is. If you know the gradient of the loss function is small then you can safely try a larger learning rate, which compensates for the small gradient and results in a larger step size."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align='center'>\n",
        "  <img src='https://developers.google.com/static/machine-learning/crash-course/images/LearningRateJustRight.svg' />\n",
        "\n",
        "  <strong>Figure 8. Learning rate is just right.</strong>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ideal learning rate in one-dimension is $\\dfrac{1}{f(x)″}$ (the inverse of the second derivative of $f(x)$ at $x$).\n",
        "\n",
        "The ideal learning rate for $2$ or more dimensions is the inverse of the [Hessian](https://wikipedia.org/wiki/Hessian_matrix) (matrix of second partial derivatives).\n",
        "\n",
        "The story for general convex functions is more complex."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stochastic Gradient Descent\n",
        "\n",
        "__Stochastic Gradient Descent (SGD)__ is an optimization algorithm used to find the minimum of a function. It is a type of gradient descent algorithm that is often used in machine learning and deep learning. It is called \"stochastic\" because it uses random samples of the data to estimate the gradient of the objective function, rather than using the entire dataset.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "* The algorithm starts with an initial guess of the parameters.\n",
        "* It then selects a random sample of the data and calculates the gradient of the objective function with respect to the parameters using that sample.\n",
        "* The parameters are then updated in the direction opposite to the gradient.\n",
        "* This process is repeated until the parameters converge to a minimum of the objective function.\n",
        "\n",
        "One of the main advantage of stochastic gradient descent is that it is computationally efficient. Because it uses a random sample of the data at each step, it can be much faster than batch gradient descent, which uses the entire dataset to calculate the gradient. Additionally, because it uses random samples, it can \"escape\" from local minima and converge to a global minimum.\n",
        "\n",
        "An example of an application of __SGD__ is Linear Regression, where the objective function is the mean squared error and the parameters are the weights of the model. Another example is Logistic Regression, where the objective function is the cross-entropy loss and the parameters are the weights of the model.\n",
        "\n",
        "It should be noted that the optimization speed of __SGD__ can be affected by the choice of the learning rate and the shuffling of the data during each iteration. Also it is not guaranteed to find the global minimum because of the randomness but it can be useful in practice.\n",
        "\n",
        "In summary, __Stochastic Gradient Descent__ is an optimization algorithm that is efficient and can help to find the global minimum of a function. It has been widely used in machine learning and deep learning tasks."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### __Check Your Understanding__\n",
        "\n",
        "__Q1. When performing gradient descent on a large data set, which of the following batch sizes will likely be more efficient?__\n",
        "\n",
        "> __A small batch or even a batch of one example (SGD).__\n",
        ">\n",
        "> Amazingly enough, performing gradient descent on a small batch or even a batch of one example is usually more efficient than the full batch. After all, finding the gradient of one example is far cheaper than finding the gradient of millions of examples. To ensure a good representative sample, the algorithm scoops up another random small batch (or batch of one) on every iteration."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to TensorFlow\n",
        "\n",
        "TensorFlow is an end-to-end open source platform for machine learning. TensorFlow is a rich system for managing all aspects of a machine learning system; however, this class focuses on using a particular TensorFlow API to develop and train machine learning models. See the [TensorFlow documentation](https://tensorflow.org/) for complete details on the broader TensorFlow system.\n",
        "\n",
        "TensorFlow APIs are arranged hierarchically, with the high-level APIs built on the low-level APIs. Machine learning researchers use the low-level APIs to create and explore new machine learning algorithms. In this class, you will use a high-level API named tf.keras to define and train machine learning models and to make predictions. tf.keras is the TensorFlow variant of the open-source [Keras](https://keras.io/) API.\n",
        "\n",
        "The following figure shows the hierarchy of TensorFlow toolkits:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div align='center'>\n",
        "  <img src='https://developers.google.com/static/machine-learning/crash-course/images/TFHierarchyNew.svg' />\n",
        "\n",
        "  <strong>Figure 1. TensorFlow toolkit hierarchy.</strong>\n",
        "</div>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear regression with tf.keras"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple Linear regression with Synthetic Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define functions that build and train a model\n",
        "\n",
        "The following code defines two functions:\n",
        "\n",
        "  * `build_model(my_learning_rate)`, which builds an empty model.\n",
        "  * `train_model(model, feature, label, epochs)`, which trains the model from the examples (feature and label) you pass. \n",
        "\n",
        "Since you don't need to understand model building code right now, you may optionally explore this code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model(learning_rate):\n",
        "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
        "  # Most simple tf.keras models are sequential. \n",
        "  # A sequential model contains one or more layers.\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Describe the topography of the model.\n",
        "  # The topography of a simple linear regression model\n",
        "  # is a single node in a single layer. \n",
        "  model.add(tf.keras.layers.Dense(units=1, \n",
        "                                  input_shape=(1,)))\n",
        "\n",
        "  # Compile the model topography into code that \n",
        "  # TensorFlow can efficiently execute. Configure \n",
        "  # training to minimize the model's mean squared error. \n",
        "  model.compile(optimizer=tf.keras.optimizers.RMSprop(\n",
        "                                    learning_rate=learning_rate),\n",
        "                loss='mean_squared_error',\n",
        "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "  return model           \n",
        "\n",
        "\n",
        "def train_model(model, feature, label, epochs, batch_size):\n",
        "  \"\"\"Train the model by feeding it data.\"\"\"\n",
        "\n",
        "  # Feed the feature values and the label values to the \n",
        "  # model. The model will train for the specified number \n",
        "  # of epochs, gradually learning how the feature values\n",
        "  # relate to the label values. \n",
        "  history = model.fit(x=feature,\n",
        "                      y=label,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epochs)\n",
        "\n",
        "  # Gather the trained model's weight and bias.\n",
        "  trained_weight = model.get_weights()[0]\n",
        "  trained_bias = model.get_weights()[1]\n",
        "\n",
        "  # The list of epochs is stored separately from the \n",
        "  # rest of history.\n",
        "  epochs = history.epoch\n",
        "  \n",
        "  # Gather the history (a snapshot) of each epoch.\n",
        "  hist = pd.DataFrame(history.history)\n",
        "\n",
        "  # Specifically gather the model's root mean \n",
        "  # squared error at each epoch. \n",
        "  rmse = hist['root_mean_squared_error']\n",
        "\n",
        "  return trained_weight, trained_bias, epochs, rmse"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define plotting functions\n",
        "\n",
        "We're using a popular Python library called [Matplotlib](https://developers.google.com/machine-learning/glossary/#matplotlib) to create the following two plots:\n",
        "\n",
        "*  a plot of the feature values vs. the label values, and a line showing the output of the trained model.\n",
        "*  a [loss curve](https://developers.google.com/machine-learning/glossary/#loss_curve)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_the_model(trained_weight, trained_bias, feature, label):\n",
        "  \"\"\"Plot the trained model against the training feature and label.\"\"\"\n",
        "\n",
        "  # Label the axes.\n",
        "  plt.xlabel('feature')\n",
        "  plt.ylabel('label')\n",
        "\n",
        "  # Plot the feature values vs. label values.\n",
        "  plt.scatter(feature, label)\n",
        "\n",
        "  # Create a red line representing the model. The red line starts\n",
        "  # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
        "  x0 = 0\n",
        "  y0 = trained_bias\n",
        "  x1 = feature[-1]\n",
        "  y1 = trained_bias + (trained_weight * x1)\n",
        "  plt.plot([x0, x1], [y0, y1], c='r')\n",
        "\n",
        "  # Render the scatter plot and the red line.\n",
        "  plt.show()\n",
        "\n",
        "def plot_the_loss_curve(epochs, rmse):\n",
        "  \"\"\"Plot the loss curve, which shows loss vs. epoch.\"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Root Mean Squared Error')\n",
        "\n",
        "  plt.plot(epochs, rmse, label='Loss')\n",
        "  plt.legend()\n",
        "  plt.ylim([rmse.min() * 0.97, rmse.max()])\n",
        "  plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the dataset\n",
        "\n",
        "The dataset consists of 12 [examples](https://developers.google.com/machine-learning/glossary/#example). Each example consists of one [feature](https://developers.google.com/machine-learning/glossary/#feature) and one [label](https://developers.google.com/machine-learning/glossary/#label)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature = ([1.0, 2.0,  3.0,  4.0,  5.0,  6.0,  7.0,  8.0,  9.0, 10.0, 11.0, 12.0])\n",
        "label   = ([5.0, 8.8,  9.6, 14.2, 18.8, 19.5, 21.4, 26.8, 28.9, 32.0, 33.8, 38.2])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specify the hyperparameters\n",
        "\n",
        "The hyperparameters in this Colab are as follows:\n",
        "\n",
        "  * [learning rate](https://developers.google.com/machine-learning/glossary/#learning_rate)\n",
        "  * [epochs](https://developers.google.com/machine-learning/glossary/#epoch)\n",
        "  * [batch_size](https://developers.google.com/machine-learning/glossary/#batch_size)\n",
        "\n",
        "The following code cell initializes these hyperparameters and then invokes the functions that build and train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate=0.01\n",
        "epochs=10\n",
        "batch_size=12\n",
        "\n",
        "model = build_model(learning_rate)\n",
        "trained_weight, trained_bias, epochs, rmse = train_model(model, feature, \n",
        "                                                         label, epochs,\n",
        "                                                         batch_size)\n",
        "plot_the_model(trained_weight, trained_bias, feature, label)\n",
        "plot_the_loss_curve(epochs, rmse)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Examine the graphs\n",
        "\n",
        "Examine the top graph. The blue dots identify the actual data; the red line identifies the output of the trained model. Ideally, the red line should align nicely with the blue dots.  Does it?  Probably not.\n",
        "\n",
        "A certain amount of randomness plays into training a model, so you'll get somewhat different results every time you train.  That said, unless you are an extremely lucky person, the red line probably *doesn't* align nicely with the blue dots.  \n",
        "\n",
        "Examine the bottom graph, which shows the loss curve. Notice that the loss curve decreases but doesn't flatten out, which is a sign that the model hasn't trained sufficiently."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: Increase the number of epochs\n",
        "\n",
        "Training loss should steadily decrease, steeply at first, and then more slowly. Eventually, training loss should eventually stay steady (zero slope or nearly zero slope), which indicates that training has [converged](http://developers.google.com/machine-learning/glossary/#convergence).\n",
        "\n",
        "In Task 1, the training loss did not converge. One possible solution is to train for more epochs.  Your task is to increase the number of epochs sufficiently to get the model to converge. However, it is inefficient to train past convergence, so don't just set the number of epochs to an arbitrarily high value.\n",
        "\n",
        "Examine the loss curve. Does the model converge?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate=0.01\n",
        "epochs=450\n",
        "batch_size=12 \n",
        "\n",
        "model = build_model(learning_rate)\n",
        "trained_weight, trained_bias, epochs, rmse = train_model(model, feature, \n",
        "                                                         label, epochs,\n",
        "                                                         batch_size)\n",
        "plot_the_model(trained_weight, trained_bias, feature, label)\n",
        "plot_the_loss_curve(epochs, rmse)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Increase the learning rate\n",
        "\n",
        "In Task 2, you increased the number of epochs to get the model to converge. Sometimes, you can get the model to converge more quickly by increasing the learning rate. However, setting the learning rate too high often makes it impossible for a model to converge. In Task 3, we've intentionally set the learning rate too high. Run the following code cell and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate=100 \n",
        "epochs=500\n",
        "batch_size = batch_size \n",
        "\n",
        "model = build_model(learning_rate)\n",
        "trained_weight, trained_bias, epochs, rmse = train_model(model, feature, \n",
        "                                                         label, epochs,\n",
        "                                                         batch_size)\n",
        "plot_the_model(trained_weight, trained_bias, feature, label)\n",
        "plot_the_loss_curve(epochs, rmse)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The resulting model is terrible; the red line doesn't align with the blue dots. Furthermore, the loss curve oscillates like a [roller coaster](https://www.wikipedia.org/wiki/Roller_coaster).  An oscillating loss curve strongly suggests that the learning rate is too high. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4: Find the ideal combination of epochs and learning rate\n",
        "\n",
        "Assign values to the following two hyperparameters to make training converge as efficiently as possible: \n",
        "\n",
        "*  `learning_rate`\n",
        "*  `epochs`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate=0.14\n",
        "epochs=70\n",
        "batch_size = batch_size\n",
        "\n",
        "model = build_model(learning_rate)\n",
        "trained_weight, trained_bias, epochs, rmse = train_model(model, feature, \n",
        "                                                         label, epochs,\n",
        "                                                         batch_size)\n",
        "plot_the_model(trained_weight, trained_bias, feature, label)\n",
        "plot_the_loss_curve(epochs, rmse)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 5: Adjust the batch size\n",
        "\n",
        "The system recalculates the model's loss value and adjusts the model's weights and bias after each **iteration**.  Each iteration is the span in which the system processes one batch. For example, if the **batch size** is 6, then the system recalculates the model's loss value and adjusts the model's weights and bias after processing every 6 examples.  \n",
        "\n",
        "One **epoch** spans sufficient iterations to process every example in the dataset. For example, if the batch size is 12, then each epoch lasts one iteration. However, if the batch size is 6, then each epoch consumes two iterations.  \n",
        "\n",
        "It is tempting to simply set the batch size to the number of examples in the dataset (12, in this case). However, the model might actually train faster on smaller batches. Conversely, very small batches might not contain enough information to help the model converge. \n",
        "\n",
        "Experiment with `batch_size` in the following code cell. What's the smallest integer you can set for `batch_size` and still have the model converge in a hundred epochs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate=0.05\n",
        "epochs=125\n",
        "batch_size=1 # Wow, a batch size of 1 works!\n",
        "\n",
        "model = build_model(learning_rate)\n",
        "trained_weight, trained_bias, epochs, rmse = train_model(model, feature, \n",
        "                                                         label, epochs,\n",
        "                                                         batch_size)\n",
        "plot_the_model(trained_weight, trained_bias, feature, label)\n",
        "plot_the_loss_curve(epochs, rmse)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of hyperparameter tuning\n",
        "\n",
        "Most machine learning problems require a lot of hyperparameter tuning.  Unfortunately, we can't provide concrete tuning rules for every model. Lowering the learning rate can help one model converge efficiently but make another model converge much too slowly.  You must experiment to find the best set of hyperparameters for your dataset. That said, here are a few rules of thumb:\n",
        "\n",
        " * Training loss should steadily decrease, steeply at first, and then more slowly until the slope of the curve reaches or approaches zero. \n",
        " * If the training loss does not converge, train for more epochs.\n",
        " * If the training loss decreases too slowly, increase the learning rate. Note that setting the learning rate too high may also prevent training loss from converging.\n",
        " * If the training loss varies wildly (that is, the training loss jumps around), decrease the learning rate.\n",
        " * Lowering the learning rate while increasing the number of epochs or the batch size is often a good combination.\n",
        " * Setting the batch size to a *very* small batch number can also cause instability. First, try large batch size values. Then, decrease the batch size until you see degradation.\n",
        " * For real-world datasets consisting of a very large number of examples, the entire dataset might not fit into memory. In such cases, you'll need to reduce the batch size to enable a batch to fit into memory. \n",
        "\n",
        "Remember: the ideal combination of hyperparameters is data dependent, so you must always experiment and verify."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Regression with a Real Dataset\n",
        "\n",
        "Now we are going to use a real dataset to predict the prices of houses in California."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Dataset\n",
        "  \n",
        "The [dataset for this exercise](https://developers.google.com/machine-learning/crash-course/california-housing-data-description) is based on 1990 census data from California. The dataset is old but still provides a great opportunity to learn about machine learning programming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# The following lines adjust the granularity of reporting. \n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The dataset\n",
        "\n",
        "Datasets are often stored on disk or at a URL in [.csv format](https://wikipedia.org/wiki/Comma-separated_values). \n",
        "\n",
        "A well-formed .csv file contains column names in the first row, followed by many rows of data.  A comma divides each value in each row. For example, here are the first five rows of the .csv file holding the California Housing Dataset:\n",
        "\n",
        "```\n",
        "\"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\",\"median_house_value\"\n",
        "-114.310000,34.190000,15.000000,5612.000000,1283.000000,1015.000000,472.000000,1.493600,66900.000000\n",
        "-114.470000,34.400000,19.000000,7650.000000,1901.000000,1129.000000,463.000000,1.820000,80100.000000\n",
        "-114.560000,33.690000,17.000000,720.000000,174.000000,333.000000,117.000000,1.650900,85700.000000\n",
        "-114.570000,33.640000,14.000000,1501.000000,337.000000,515.000000,226.000000,3.191700,73400.000000\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the .csv file into a pandas DataFrame\n",
        "\n",
        "Like many machine learning programs, we gather the `.csv` file and stores the data in memory as a pandas Dataframe. Pandas is an open source Python library. The primary datatype in pandas is a DataFrame.  You can imagine a pandas DataFrame as a spreadsheet in which each row is identified by a number and each column by a name. Pandas is itself built on another open source Python library called NumPy.\n",
        "\n",
        "The following code cell imports the .csv file into a pandas DataFrame and scales the values in the label (`median_house_value`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the dataset.\n",
        "training_df = pd.read_csv(filepath_or_buffer=\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
        "\n",
        "# Scale the label.\n",
        "training_df[\"median_house_value\"] /= 1000.0\n",
        "\n",
        "# Print the first rows of the pandas DataFrame.\n",
        "training_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scaling `median_house_value` puts the value of each house in units of thousands. Scaling will keep loss values and learning rates in a friendlier range.  \n",
        "\n",
        "Although scaling a label is usually *not* essential, scaling features in a multi-feature model usually *is* essential."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Examine the dataset\n",
        "\n",
        "A large part of most machine learning projects is getting to know your data. The pandas API provides a `describe` function that outputs the following statistics about every column in the DataFrame:\n",
        "\n",
        "* `count`, which is the number of rows in that column. Ideally, `count` contains the same value for every column. \n",
        "\n",
        "* `mean` and `std`, which contain the mean and standard deviation of the values in each column. \n",
        "\n",
        "* `min` and `max`, which contain the lowest and highest values in each column.\n",
        "\n",
        "* `25%`, `50%`, `75%`, which contain various [quantiles](https://developers.google.com/machine-learning/glossary/#quantile)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get statistics on the dataset.\n",
        "training_df.describe()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1: Identify anomalies in the dataset\n",
        "\n",
        "Do you see any anomalies (strange values) in the data?\n",
        "\n",
        "> The maximum value (max) of several columns seems very high compared to the other quantiles. For example, example the total_rooms column. Given the quantile values (25%, 50%, and 75%), you might expect the max value of total_rooms to be approximately 5,000 or possibly 10,000. However, the max value is actually 37,937.\n",
        ">\n",
        "> When you see anomalies in a column, become more careful about using that column as a feature. That said, anomalies in potential features sometimes mirror anomalies in the label, which could make the column be (or seem to be) a powerful feature."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define functions that build and train a model\n",
        "\n",
        "The following code defines two functions:\n",
        "\n",
        "  * `build_model(my_learning_rate)`, which builds a randomly-initialized model.\n",
        "  * `train_model(model, feature, label, epochs)`, which trains the model from the examples (feature and label) you pass. \n",
        "\n",
        "Since you don't need to understand model building code right now, you may optionally explore this code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model(my_learning_rate):\n",
        "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
        "  # Most simple tf.keras models are sequential.\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Describe the topography of the model.\n",
        "  # The topography of a simple linear regression model\n",
        "  # is a single node in a single layer.\n",
        "  model.add(tf.keras.layers.Dense(units=1, \n",
        "                                  input_shape=(1,)))\n",
        "\n",
        "  # Compile the model topography into code that TensorFlow can efficiently\n",
        "  # execute. Configure training to minimize the model's mean squared error. \n",
        "  model.compile(optimizer=tf.keras.optimizers.RMSprop(\n",
        "                              learning_rate=my_learning_rate),\n",
        "                loss=\"mean_squared_error\",\n",
        "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "  return model        \n",
        "\n",
        "\n",
        "def train_model(model, df, feature, label, epochs, batch_size):\n",
        "  \"\"\"Train the model by feeding it data.\"\"\"\n",
        "\n",
        "  # Feed the model the feature and the label.\n",
        "  # The model will train for the specified number of epochs. \n",
        "  history = model.fit(x=df[feature],\n",
        "                      y=df[label],\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epochs)\n",
        "\n",
        "  # Gather the trained model's weight and bias.\n",
        "  trained_weight = model.get_weights()[0]\n",
        "  trained_bias = model.get_weights()[1]\n",
        "\n",
        "  # The list of epochs is stored separately from the rest of history.\n",
        "  epochs = history.epoch\n",
        "  \n",
        "  # Isolate the error for each epoch.\n",
        "  hist = pd.DataFrame(history.history)\n",
        "\n",
        "  # To track the progression of training, we're going to take a snapshot\n",
        "  # of the model's root mean squared error at each epoch. \n",
        "  rmse = hist[\"root_mean_squared_error\"]\n",
        "\n",
        "  return trained_weight, trained_bias, epochs, rmse"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define plotting functions\n",
        "\n",
        "We're using a popular Python library called [Matplotlib](https://developers.google.com/machine-learning/glossary/#matplotlib) to create the following two plots:\n",
        "\n",
        "*  a plot of the feature values vs. the label values, and a line showing the output of the trained model.\n",
        "*  a [loss curve](https://developers.google.com/machine-learning/glossary/#loss_curve)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_the_model(trained_weight, trained_bias, feature, label):\n",
        "  \"\"\"Plot the trained model against 200 random training examples.\"\"\"\n",
        "\n",
        "  # Label the axes.\n",
        "  plt.xlabel(feature)\n",
        "  plt.ylabel(label)\n",
        "\n",
        "  # Create a scatter plot from 200 random points of the dataset.\n",
        "  random_examples = training_df.sample(n=200)\n",
        "  plt.scatter(random_examples[feature], random_examples[label])\n",
        "\n",
        "  # Create a red line representing the model. The red line starts\n",
        "  # at coordinates (x0, y0) and ends at coordinates (x1, y1).\n",
        "  x0 = 0\n",
        "  y0 = trained_bias\n",
        "  x1 = random_examples[feature].max()\n",
        "  y1 = trained_bias + (trained_weight * x1)\n",
        "  plt.plot([x0, x1], [y0, y1], c='r')\n",
        "\n",
        "  # Render the scatter plot and the red line.\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_the_loss_curve(epochs, rmse):\n",
        "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Root Mean Squared Error\")\n",
        "\n",
        "  plt.plot(epochs, rmse, label=\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.ylim([rmse.min() * 0.97, rmse.max()])\n",
        "  plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Call the model functions\n",
        "\n",
        "An important part of machine learning is determining which [features](https://developers.google.com/machine-learning/glossary/#feature) correlate with the [label](https://developers.google.com/machine-learning/glossary/#label). For example, real-life home-value prediction models typically rely on hundreds of features and synthetic features. However, this model relies on only one feature. For now, you'll arbitrarily use `total_rooms` as that feature. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.01\n",
        "epochs = 30\n",
        "batch_size = 30\n",
        "\n",
        "# Specify the feature and the label.\n",
        "feature = \"total_rooms\"  # the total number of rooms on a specific city block.\n",
        "label=\"median_house_value\" # the median value of a house on a specific city block.\n",
        "# That is, you're going to create a model that predicts house value based \n",
        "# solely on total_rooms.  \n",
        "\n",
        "# Discard any pre-existing version of the model.\n",
        "model = None\n",
        "\n",
        "# Invoke the functions.\n",
        "model = build_model(learning_rate)\n",
        "weight, bias, epochs, rmse = train_model(model, training_df, \n",
        "                                         feature, label,\n",
        "                                         epochs, batch_size)\n",
        "\n",
        "print(\"\\nThe learned weight for your model is %.4f\" % weight)\n",
        "print(\"The learned bias for your model is %.4f\\n\" % bias )\n",
        "\n",
        "plot_the_model(weight, bias, feature, label)\n",
        "plot_the_loss_curve(epochs, rmse)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A certain amount of randomness plays into training a model. Consequently, you'll get different results each time you train the model. That said, given the dataset and the hyperparameters, the trained model will generally do a poor job describing the feature's relation to the label."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use the model to make predictions\n",
        "\n",
        "You can use the trained model to make predictions. In practice, [you should make predictions on examples that are not used in training](https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data). However, for this exercise, you'll just work with a subset of the same training dataset.\n",
        "\n",
        "First, run the following code to define the house prediction function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_house_values(n, feature, label):\n",
        "  \"\"\"Predict house values based on a feature.\"\"\"\n",
        "\n",
        "  batch = training_df[feature][10000:10000 + n]\n",
        "  predicted_values = model.predict_on_batch(x=batch)\n",
        "\n",
        "  print(\"feature   label          predicted\")\n",
        "  print(\"  value   value          value\")\n",
        "  print(\"          in thousand$   in thousand$\")\n",
        "  print(\"--------------------------------------\")\n",
        "  for i in range(n):\n",
        "    print (\"%5.0f %6.0f %15.0f\" % (training_df[feature][10000 + i],\n",
        "                                   training_df[label][10000 + i],\n",
        "                                   predicted_values[i][0] ))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, invoke the house prediction function on 10 examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predict_house_values(10, feature, label)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2: Judge the predictive power of the model\n",
        "\n",
        "Look at the preceding table. How close is the predicted value to the label value?  In other words, does your model accurately predict house values?\n",
        "\n",
        "> Most of the predicted values differ significantly from the label value, so the trained model probably doesn't have much predictive power. However, the first 10 examples might not be representative of the rest of the examples.  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Try a different feature\n",
        "\n",
        "The `total_rooms` feature had only a little predictive power. Would a different feature have greater predictive power?  Try using `population` as the feature instead of `total_rooms`. \n",
        "\n",
        "Note: When you change features, you might also need to change the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pick a feature other than \"total_rooms\"\n",
        "feature = \"population\"\n",
        "\n",
        "# Possibly, experiment with the hyperparameters.\n",
        "learning_rate = 0.05\n",
        "epochs = 18\n",
        "batch_size = 3\n",
        "\n",
        "# Don't change anything below.\n",
        "model = build_model(learning_rate)\n",
        "weight, bias, epochs, rmse = train_model(model, training_df, \n",
        "                                         feature, label,\n",
        "                                         epochs, batch_size)\n",
        "\n",
        "plot_the_model(weight, bias, feature, label)\n",
        "plot_the_loss_curve(epochs, rmse)\n",
        "\n",
        "predict_house_values(10, feature, label)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Did `population` produce better predictions than `total_rooms`?\n",
        "\n",
        "> Training is not entirely deterministic, but population typically converges at a slightly higher RMSE than total_rooms. So, population appears to be about the same or slightly worse at making predictions than total_rooms."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4: Define a synthetic feature\n",
        "\n",
        "You have determined that `total_rooms` and `population` were not useful features.  That is, neither the total number of rooms in a neighborhood nor the neighborhood's population successfully predicted the median house price of that neighborhood. Perhaps though, the *ratio* of `total_rooms` to `population` might have some predictive power. That is, perhaps block density relates to median house value.\n",
        "\n",
        "To explore this hypothesis, do the following: \n",
        "\n",
        "1. Create a [synthetic feature](https://developers.google.com/machine-learning/glossary/#synthetic_feature) that's a ratio of `total_rooms` to `population`.\n",
        "2. Tune the three hyperparameters.\n",
        "3. Determine whether this synthetic feature produces \n",
        "   a lower loss value than any of the single features you \n",
        "   tried earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a synthetic feature\n",
        "training_df[\"rooms_per_person\"] = training_df[\"total_rooms\"] / training_df[\"population\"]\n",
        "feature = \"rooms_per_person\"\n",
        "\n",
        "# Tune the hyperparameters.\n",
        "learning_rate = 0.06\n",
        "epochs = 24\n",
        "batch_size = 30\n",
        "\n",
        "# Don't change anything below this line.\n",
        "model = build_model(learning_rate)\n",
        "weight, bias, epochs, mae = train_model(model, training_df,\n",
        "                                        feature, label,\n",
        "                                        epochs, batch_size)\n",
        "\n",
        "plot_the_model(weight, bias, feature, label)\n",
        "plot_the_loss_curve(epochs, mae)\n",
        "predict_house_values(15, feature, label)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the loss values, this synthetic feature produces a better model than the individual features you tried in Task 2 and Task 3. However, the model still isn't creating great predictions."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 5. Find feature(s) whose raw values correlate with the label\n",
        "\n",
        "So far, we've relied on trial-and-error to identify possible features for the model.  Let's rely on statistics instead.\n",
        "\n",
        "A **correlation matrix** indicates how each attribute's raw values relate to the other attributes' raw values. Correlation values have the following meanings:\n",
        "\n",
        "  * `1.0`: perfect positive correlation; that is, when one attribute rises, the other attribute rises.\n",
        "  * `-1.0`: perfect negative correlation; that is, when one attribute rises, the other attribute falls. \n",
        "  * `0.0`: no correlation; the two columns [are not linearly related](https://en.wikipedia.org/wiki/Correlation_and_dependence#/media/File:Correlation_examples2.svg).\n",
        "\n",
        "In general, the higher the absolute value of a correlation value, the greater its predictive power. For example, a correlation value of -0.8 implies far more predictive power than a correlation of -0.2.\n",
        "\n",
        "The following code cell generates the correlation matrix for attributes of the California Housing Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a correlation matrix.\n",
        "training_df.corr()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The correlation matrix shows nine potential features (including a synthetic\n",
        "feature) and one label (`median_house_value`).  A strong negative correlation or strong positive correlation with the label suggests a potentially good feature.  \n",
        "\n",
        "**Your Task:** Determine which of the nine potential features appears to be the best candidate for a feature?\n",
        "\n",
        "> The median_income correlates 0.7 with the label (median_house_value), so median_income might be a good feature. The other seven potential features all have a correlation relatively close to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature = \"median_income\"\n",
        "\n",
        "# Possibly, experiment with the hyperparameters.\n",
        "learning_rate = 0.01\n",
        "epochs = 10\n",
        "batch_size = 3\n",
        "\n",
        "# Don't change anything below.\n",
        "model = build_model(learning_rate)\n",
        "weight, bias, epochs, rmse = train_model(model, training_df, \n",
        "                                         feature, label,\n",
        "                                         epochs, batch_size)\n",
        "\n",
        "plot_the_model(weight, bias, feature, label)\n",
        "plot_the_loss_curve(epochs, rmse)\n",
        "\n",
        "predict_house_values(10, feature, label)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "d6f2081d9e0b7d32ecec298930d2c84146e44d12c2a59792b9d51f0a2535e83c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
